mport torch
import numpy as np
import torch.nn as nn
import torch.nn.function as f
'''x=torch.rand(3,5)
y=torch.Tensor(3,5)#未被初始化
z=torch.rand(3,5)
print(x+y)

print(torch.add(x,y))
result=torch.rand(3,5)
print(torch.add(x,y,out=result))
print(x.add_(y))
#Tensor和numpy数组转换很容易，但是修改一个会导致另一个的修改
a=torch.ones(3,5)
b=a.numpy()
b=b+1
print(a,b)

a=np.ones((3,5))
b=torch.from_numpy(a)
#torch.add(a,1,out=a),torch.add只能进行tensor之间的加法
np.add(a,1,out=a)
print(a)
print(b)
x,y=torch.ones((3,5)),torch.ones((3,5))
if torch.cuda.is_available():
    x = x.cuda()
    y = y.cuda()
    print(x + y)
from torch.autograd import Variable
x=Variable(torch.ones(2,2),requires_grad=True)
y=x+1
y=y.creator

z=y*y*2

out=z.mean()
out.backward()
print(x.data)
print(y.grad)

import torch
from torch.autograd import Variable
t1=torch.Tensor([1,2,3])
a=Variable(t1,requires_grad=True)
t2=torch.Tensor([2,3,4])
b=Variable(t2,requires_grad=True)
#所以什么呢，.backward不带参数时c必须是一维的标量，否则应当给出参数
c=a+b
k=c*4
print(k)
#c_=c.detach()
k.backward(torch.Tensor([1,2,3]))
#print(a.grad_fn)
#print(c.grad_fn)
